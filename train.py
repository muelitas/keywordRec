'''/****************************************************************************
    File: train.py
    Author(s): Mario Esparza
    Date: 03/10/2022
    Description: Grab data, train model, record logs, save plots and 
        checkpoints.
    Updates: N/A
        
*****************************************************************************'''

import copy 
import gc
import math
import os.path as os_path
import sklearn.model_selection as sklearn_ms
import sys
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data

import utils.funcs as uf
from utils.classes import Lr_Sched, Manager, Custom_Dataset, Metrics, Logger
from utils.models import SpeechRecognitionModel

class Trainer(Manager):
    def __init__(self):
        super().__init__()

        self.gt_path = "" #Path to ground truth
        self.phonemes_path = ""
        self.hyperparams_json = ""
        self.otherparams_json = ""
        self.produced_files_path = ""
        self.chckpnt_path = ""

        self.train_log = None
        self.misc_log = None

        self.train_dataset = None
        self.dev_dataset = None #for validation
        self.train_dataloader = None
        self.dev_dataloader = None

        self.curr_hp = {} #hyperparameters in current grid
        self.model = None
        self.model_num_params = None
        self.optimizer = None
        self.criterion = None
        self.scheduler = None

    def check_user_input(self, user_input):
        '''Make sure user is providing the right input'''
        try:
            self.gt_path, self.phonemes_path, self.hyperparams_json, \
                self.otherparams_json, self.produced_files_path = user_input.split(' ')[1:6]
        except Exception as e:
            print(f"Error: {e}")

    def set_paths(self):
        '''Create paths to files that will be generated by the trainer, like 
        logs, checkpoints, etc.'''
        self.chckpnt_path = os_path.join(self.produced_files_path, "chkpnt.tar")

        msg = "Training and Validation results will be saved here: "
        self.train_log = Logger(self.produced_files_path, "training.log", msg)
    
        msg = "HyperParams, Models' Summaries and other miscellaneous info "
        msg += "will be saved here: "
        self.misc_log = Logger(self.produced_files_path, "miscelaneous.log", msg)

    def initialize_params(self):
        '''Initialize hyper parameters, 'other' parameters, cuda device,
        loag paths, etc. Perform memory clean up as well (to start fresh).'''
        self.set_hyperparams(self.hyperparams_json)
        self.set_otherparams(self.otherparams_json)
        self.set_cuda_and_device() #CPU or GPU?
        self.set_paths()
        self.clean_up() #Make sure memory is empty before starting

    def initialize_datasets(self, ):
        self.train_dataset = Custom_Dataset(
            self.gt_path, 
            self.ipa2char, 
            self.OP["split_ratio"],
            "front"
        )
        self.dev_dataset = Custom_Dataset(
            self.gt_path, 
            self.ipa2char, 
            self.OP["split_ratio"],
            "back"
        )

    def set_data_loaders(self):
        self.train_dataloader = data.DataLoader(
            dataset=self.train_dataset,
            batch_size=self.curr_hp['bs'],
            shuffle=True,
            collate_fn=lambda x: uf.data_processing(
                x, 
                self.char2int, 
                self.OP["spec_augment_fm"], 
                self.OP["spec_augment_tm"], 
                self.OP["spec_augment"]
            ),
            **self.data_loader_kwargs
        )

        self.dev_dataloader = data.DataLoader(
            dataset=self.dev_dataset,
            batch_size=self.curr_hp['bs'],
            shuffle=True,
            collate_fn=lambda x: uf.data_processing(x, self.char2int),
            **self.data_loader_kwargs
        )
    
    def set_model(self):
        '''Declare model, optimizer, loss function, scheduler, etc.'''
        self.model = SpeechRecognitionModel(self.curr_hp, self.OP["n_class"]).to(self.device)
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            self.curr_hp['e_0']
        )
        self.criterion = nn.CTCLoss(blank=self.OP["blank_label"]).to(self.device)
        self.scheduler = Lr_Sched(self.curr_hp)
        self.metrics = Metrics()

    def get_loss(self, output, labels, input_lengths, label_lengths):
        loss = None
        try:
            loss = self.criterion(output, labels, input_lengths, label_lengths)
        except Exception as exception:
            m = f"ERROR in Trainer.getloss. Exception is: {exception}. "
            m += f"labels = {labels}. input_lengths = {input_lengths}. "
            m += f"label_lengths = {label_lengths}\n"
            self.train_log.log_msg(m, 'a', True, "RED")
            sys.exit()
        
        return loss

    def train_model(self, epoch):
        self.model.train()
        self.train_log.log_msg(f"\tTraining Epoch #{epoch} | ", 'a', True)
        
        train_losses, lrs, train_per = [], [], []
        for batch_idx, _data in enumerate(self.train_dataloader):
            spectrograms, labels, input_lengths, label_lengths, filenames = _data
            spectrograms = spectrograms.to(self.device)
            labels = labels.to(self.device)

            self.optimizer.zero_grad()
            
            self.model.add_paddings(spectrograms)
            output = self.model(spectrograms)  # (batch, time, n_class)
            #If I want to get probabilities, use F.softmax here.
            #Reference: https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
            output = F.log_softmax(output, dim=2)
            output = output.transpose(0, 1) # (time, batch, n_class)

            loss = self.get_loss(output, labels, input_lengths, label_lengths)
            train_losses.append(loss.detach().item())
            loss.backward()
            
            decoded_preds, decoded_targets = uf.greedy_decoder(
                output.transpose(0, 1),
                labels, 
                label_lengths, 
                self.OP["blank_label"], 
                self.int2char
            )
            
            for j in range(len(decoded_preds)):
                    train_per.append(uf.cer(decoded_targets[j], decoded_preds[j]))
        
            lrs.append(self.optimizer.param_groups[0]['lr'])
            self.optimizer.step()
        
        avg_per = sum(train_per)/len(train_per)        
        train_loss = sum(train_losses) / len(train_losses) #epoch's average loss
        avg_lr = sum(lrs) / len(lrs)
        self.metrics.add_train_metrics(train_loss, avg_per, avg_lr)
        
        self.train_log.log_msg(f"Avg Loss: {train_loss:.4f}\n", 'a', True)
        
        #Update learning rate
        new_lr = self.scheduler.step() 
        self.optimizer.param_groups[0]['lr'] = new_lr

    def validate_model(self, epoch):
        #used to print decoded predictions and targets
        batches_num = math.ceil(self.dev_dataloader.batch_sampler.sampler.num_samples / 
                                self.dev_dataloader.batch_size)
        step = (batches_num // 2) + 1
    
        self.model.eval()
        dev_losses, dev_per, dev_wer = [], [], []
        decoded_preds, decoded_targets, MSG = [], [], ''

        self.train_log.log_msg(f"\tDev Epoch #{epoch} | ", 'a', True)
        with torch.no_grad():
            for i, _data in enumerate(self.dev_dataloader):
                spectrograms, labels, input_lengths, label_lengths, _ = _data 
                spectrograms = spectrograms.to(self.device)
                labels = labels.to(self.device)
                
                self.model.add_paddings(spectrograms)
                output = self.model(spectrograms)  # (batch, time, n_class)
                output = F.log_softmax(output, dim=2)
                output = output.transpose(0, 1) # (time, batch, n_class)

                loss = self.get_loss(output, labels, input_lengths, label_lengths)
                dev_losses.append(loss.detach().item())

                decoded_preds, decoded_targets = uf.greedy_decoder(
                    output.transpose(0, 1),
                    labels, 
                    label_lengths, 
                    self.OP["blank_label"], 
                    self.int2char
                )
                
                #Decode prediction and targets into ipa phonemes
                predicted_ipas, target_ipas = uf.chars_to_ipa(
                    decoded_preds,
                    decoded_targets, 
                    self.char2ipa
                )
                                    
                #Log some more predictions
                if i % step == 0:
                    for k in range(0,len(spectrograms)):
                        MSG += f'\t\tBatch [{i+1}-{k}/{batches_num}] | '
                        MSG += f"{target_ipas[k]}' -> '{predicted_ipas[k]}'\n"
                
                for j in range(len(decoded_preds)):
                    dev_per.append(uf.cer(decoded_targets[j], decoded_preds[j]))
                    dev_wer.append(uf.wer(decoded_targets[j], decoded_preds[j]))

        avg_per = sum(dev_per)/len(dev_per)
        avg_wer = sum(dev_wer)/len(dev_wer)
        dev_loss = sum(dev_losses) / len(dev_losses) #epoch's average loss
        ratio_loss = dev_loss / self.metrics.train_losses[epoch-1]

        m = f"Avg Loss: {dev_loss:.4f} | Ratio Loss: {ratio_loss:.4f} | "
        m+= f"Avg PER: {avg_per:.4f} | Avg WER: {avg_wer:.4f}\n"
        self.train_log.log_msg(m, 'a', True)
        self.train_log.log_msg(MSG, 'a', True)
        self.metrics.add_dev_metrics(dev_loss, avg_per, ratio_loss)

    def save_model(self, param_grid_idx, epoch):
        '''Determine if it was a good run, if we should save checkpoint and if
        we should stop training.''' 
        #Check if ratio loss is between 1.02 and 0.98
        keep_it = self.metrics.keep_RL_result()
        
        #If {keep_it} is True and current PER is lower than
        #{RL_global_best_per}, copy model
        epoch_per = self.metrics.dev_pers[-1]
        if epoch_per < self.metrics.RL_global_best_per and keep_it:
            self.metrics.RL_global_best_per = epoch_per
            self.metrics.RL_best_model_wts = copy.deepcopy(self.model.state_dict())
            # optimizer_state_dict = copy.deepcopy(optimizer.state_dict())
            self.metrics.RL_best_hparams = self.curr_hp
            self.metrics.RL_run_num = str(param_grid_idx)
            self.metrics.RL_epoch_num = str(epoch)
        
        #If current PER is less than {global_best_per}, copy model
        if epoch_per < self.metrics.global_best_per:
            self.metrics.global_best_per = epoch_per
            self.metrics.best_model_wts = copy.deepcopy(self.model.state_dict())
            # optimizer_state_dict = copy.deepcopy(optimizer.state_dict())
            self.metrics.best_hparams = self.curr_hp
            self.metrics.run_num = str(param_grid_idx)
            self.metrics.epoch_num = str(epoch)
        
        return self.metrics.should_we_stop(epoch, self.OP)

    def clean_up(self):
        '''Delete model, collect garbage and empty CUDA memory'''
        #See: https://stackify.com/python-garbage-collection/
        # model.apply(weights_init)
        del self.model, self.criterion, self.optimizer, self.scheduler
        first_gc = gc.collect()
        torch.cuda.empty_cache()
        second_gc = gc.collect()
        m = f"Cleanup results: | First value of gc = {first_gc}. | "
        m += f"Second value of gc = {second_gc}\n"
        self.misc_log.log_msg(m, 'a', False)

    def save_checkpoint(self, model_wts, hparams, chkpnt_path, run_num, epoch_num,
                     case):
        
        if run_num != '-1' and epoch_num != '-1':
            save_path = chkpnt_path[:-4] + f'_OnRun{run_num.zfill(2)}'
            save_path += f'Epoch{epoch_num.zfill(3)}Case{case}' + chkpnt_path[-4:]
                
            #This is the best model of all epochs and all runs
            torch.save({
                'model_state_dict': model_wts,
                # 'optimizer_state_dict': optimizer_state_dict,
                'hparams': hparams
            }, save_path)
        else:
            m = ""
            if case == 'YesRL':
                m = "\nMODEL DIDN'T HAVE A RATIO LOSS BETWEEN 1.02 AND 0.98. "
                m += "I AM NOT SAVING THE MODEL.\n"
            else:
                m = "\nFOR SOME REASON, I DIDN'T NOT GET A BEST MODEL "
                m += "(INDEPENDENDENT OF THE RATIO LOSS). CHECK IT OUT.\n"

            self.misc_log.log_msg(m, 'a', False)
            save_path = f'ModelNotSaved_{case}'
        
        return save_path

    def save_checkpoints(self):
        '''Save models's weights and hyper parameters. I am saving two checkpoints
        1) one in which the model reaches a low PER and great Ratio Loss (between
        1.02 and 0.98) 2) and another one in which bestest PER is achieved (not
        caring about the ration loss).'''
        #Save the one dependent on the Ratio Loss
        chckpnt_path_RL = self.save_checkpoint(
            self.metrics.RL_best_model_wts, 
            self.metrics.RL_best_hparams, 
            self.chckpnt_path, 
            self.metrics.RL_run_num, 
            self.metrics.RL_epoch_num,
            'YesRL'
        )
        #And the one independent of the Ratio Loss
        chckpnt_path_PER = self.save_checkpoint(
            self.metrics.best_model_wts, 
            self.metrics.best_hparams, 
            self.chckpnt_path,
            self.metrics.run_num, 
            self.metrics.epoch_num,
            'NoRL'
        )
        return chckpnt_path_RL, chckpnt_path_PER
    
    def log_overall_results(self, chckpnt_PER, chckpnt_RL):
        sm = self.metrics #To save some typing
        #Record "bestest" metrics, run-time, and others
        m = f"\nBest PER of all was {min(sm.best_pers):.4f} on run "
        m += f"{sm.best_pers.index(min(sm.best_pers)) + 1}\n"
        m += f"chckpnt_path_PER has been saved here: {chckpnt_PER}\n"
        m += f"chckpnt_path_RL has been saved here: {chckpnt_RL}\n"
        m += f"Number of parameters in model: {self.model_num_params}\n"
        m += f"In all runs, training set had {len(self.train_dataset)} audio files\n"
        # msg += f"equivalent to {self.train_dataset.duration:.2f} seconds\n"
        m += f"In all runs, dev set had {len(self.dev_dataset)} audio files\n"
        # msg += f"valent to {dev_dataset.duration:.2f} seconds\n"
        m += f"Early Stop Values:\n\tn: {self.OP['early_stop_n']}\n\tPercentage: "
        m += f"{((1-self.OP['early_stop_p'])*100):.2f}%\n\tOverfit Threshold: "
        m += f"{self.OP['early_stop_t']:.2f}\n\tNumber of epochs to wait: "
        m += f"{self.OP['early_stop_w']}\n"
        m += f"Number of classes: {self.OP['n_class']}\n"
        m += f"Are we using masking during training? {self.OP['spec_augment']}\n"
        if self.OP['spec_augment']:
            m += f"Time Masking Coeff.: {self.OP['spec_augment_tm']}, "
            m += f"Frequency Masking: {self.OP['spec_augment_fm']}\n"
        self.train_log.log_msg(m, 'a', True)

    def log_param_grid_instance_per(self, stop_msg):
        local_best_per = self.metrics.get_best_cer()
        self.metrics.best_pers.append(local_best_per)
        
        m = stop_msg + f"Best PER: {local_best_per:.4f} on Epoch "
        m += f"{self.metrics.dev_pers.index(local_best_per) + 1}\n"
        self.train_log.log_msg(m, "a", True)

    def log_param_grid_instance(self, instance_number):
        num_runs = len(list(sklearn_ms.ParameterGrid(self.HP)))
        
        m = f"ParameterGrid Case [{instance_number}/{num_runs}]\n"
        self.train_log.log_msg(m, 'a', True)

        m = f"----------PARAMETERS [{instance_number}/{num_runs}]----------\n"
        self.misc_log.log_msg(m, 'a', False)

    def log_model_information(self):
        '''Log model summary, # of parameters, hyper parameters and more'''
        #Change location of stdout to print everything in {misc_log.log_path}
        original_stdout = sys.stdout # Save a reference of original stdout
        with open(self.misc_log.log_path, 'a') as log:
            self.model_num_params = sum([param.nelement() for param in \
                self.model.parameters() if param.requires_grad])
            sys.stdout = log # Change standard output to {misc_log.log_path}
            
            #Number of parameters
            print('Total Number of Parameters in the Model is: ',end='')
            print(self.model_num_params)
            
            #Model Summary
            print("\nModel's Summary:")
            print(self.model) #print model summary
            
            #Hyper Parameters
            print('\nHyper Parameters:')
            for k,v in self.curr_hp.items():
                print(f'\t{k}: {v}')
            
            print(f"\nNumber of Classes/Labels: {self.OP['n_class']}")
            print('\n')

            #Reset std output to its original value (the terminal)
            sys.stdout = original_stdout

    def log_labels_mappings(self):
        '''Log which integer (label) represented each phoneme.'''
        m = '\nThe label representation for all runs was the following:\n'
        m += '  int char  IPA\n'
        for [k1, v1], [_, v2] in zip(self.ipa2char.items(), self.char2int.items()):
            m += f"{v2:>5}{v1:>5}{k1:>5}\n"
        
        m += f"'Blank Label': {len(self.ipa2char.items())}\n"
        self.misc_log.log_msg(m, 'a', False)
        
    def train(self):
        for idx, hparams in enumerate(list(sklearn_ms.ParameterGrid(self.HP))):
            start_time = time.process_time()
            self.log_param_grid_instance(idx+1)
            try:
                self.curr_hp = hparams
                self.set_data_loaders()
                self.set_model() #as well as optimizer and criterion

                for epoch in range(1, self.curr_hp['epochs'] + 1):
                    self.train_model(epoch)
                    self.validate_model(epoch)
                    stop, stop_msg = self.save_model(idx+1, epoch)
                    if stop: 
                        break
                    
                self.log_param_grid_instance_per(stop_msg)
                self.log_model_information()
                self.train_log.save_figs(self.metrics, idx+1)

            except Exception as e:
                m = f"ERROR in function Trainer.train: '{e}'\n"
                self.train_log.log_msg(m, 'a', True, "RED")

            finally:
                self.clean_up()

            m = f"Run took {(time.process_time() - start_time):.2f} seconds\n"
            self.train_log.log_msg(m, "a", True)
        
        chckpnt_path_RL, chckpnt_path_PER = self.save_checkpoints()
        self.log_overall_results(chckpnt_path_PER, chckpnt_path_RL)
        self.log_labels_mappings()